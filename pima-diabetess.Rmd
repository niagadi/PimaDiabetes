---
title: "Physiological Characteristics of T2DM in Pima Indians"
author: "Nirupa Gadi"
date: "June/2020"
output: html_document
---

## Purpose 

The purpose of this study is to develop a regression model that can predict incidence of Type II Diabetes Mellitus (T2DM) in a population of Pima Indians. Four different regression approaches will be attempted to yield an optimised accuracy with sensitivity and specificity measures.

## Background

Over thousands of epidemiological studies, researchers and healthcare providers have gathered tremendous information on the healthcare disparities that affect Native Americans. Previous research has shown that this population unfortunately suffers from excessive burden of disease and lower life expectancies. These occurences are perhaps due to high rates of poverty, systemic racism, and many other forms of health discrimination based on factors of intersectionality. 

Sources have shown that Native Americans can suffer up to three times the rate of Type II Diabetes Mellitus (T2DM) compared to Caucasian and coloured Americans. This high incidence of disease can be problematic, as proper management of T2DM relies on education and strict self-management. If poorly managed, T2DM may lead to many other adverse health outcomes such as severe heart disease or COVID-19 mortality. 

To limit the rates of these diseases, prevention on a primary and secondary level is preferred to life-long treatment. In order to do this, healthcare providers must have robust methods of predicting which patients would likely have the disease. To anticipate  risk of diabetes, development of a robust model of regression is necessary.

This study is focused on analysis of the `Diabetes` dataset downloaded to a personal computer from Kaggle at: https://www.kaggle.com/uciml/pima-indians-diabetes-database 
The dataset `Diabetes` relates frequencies and statistics of physiological measurements on women over the age of 21 belonging to the Pima Native American tribe residing of Arizona. 

## Exploratory Data Analysis
### Setup

Below is information to detail the operating system. 

```{r}
#Check computer details#
version
```

The dataset has been downloaded from Kaggle and stored on a personal computer, where it will be pushed into R. 

```{r}
#Download the dataset from an excel sheet on computer#
if(!require(readxl)) install.packages("readxl")
library(readxl)
file.exists("/Users/nerp/Desktop/diabetes.xlsx")
pima<- read_xlsx("/Users/nerp/Desktop/diabetes.xlsx")
```

After upload, the initial structure of the `pima` dataset should be studied.

```{r}
#Check the first 6 rows of the provided dataset#
head(pima)
```

`pima` appears to be in tidy format, meaning that each variable forms a column, each observation is a row, and the observational unit forms a table. Next, the parameters within the dataset will be defined. 

```{r}
#See the overall structure of the dataset#
str(pima)
```

It appears that `pima` has 768 observations of 9 variables, all of which are in numeric format. 
`Pregnancies` describes the gravidity of each patient in the dataset
`Glucose` describes the mg of glucose per every dL of blood in the patient
`BloodPressure` describes the diastolic blood pressure of each patient
`SkinThickness` measures the epidermal, dermal, and subcutaneous layers of brachial skin for each patient
`Insulin` describes the mIU of insulin protein per litre of blood after 2 hours of fasting
`BMI` describes the standardised body mass index of each patient. 
`DiabetesPedigreeFunction` describes the result of an unlisted function that calculates the genetic influence of T2DM in each patient.
`Age` describes the rounded age in years of each patient
`Outcome` denotes `0` for non-diabetic and `1` for diabetic for each patient

Next, missing values are quite common in real-life datasets, so it is imperative to inspect the set for this.

```{r}
#Find the proportion of the dataset that is NA#
sum(is.na(pima))
sum(is.na(pima))/(ncol(pima)*nrow(pima))
```

There are no missing values in the `pima` dataset, which makes up `0`% of the total set. This is very impressive for clinical data analysis, as medical records tend to have missing data.

Now that the data has been cleaned up, it is ready for some visualisation analysis. 

### Visualisation 

Visualisation is a key step before modelling. Generating plots enables the data scientist to have an overall understand of trends in the data, faciliating the  analysis.

```{r}
#Download commonly needed packages for visualisation#
if (!require(plyr)) install.packages("plyr")
if (!require(ggrepel)) install.packages("ggrepel")
if (!require(gridExtra)) install.packages("gridExtra")
library(plyr)
library(dplyr)
library(gridExtra)
```

There are 8 parameters that could be possibly correlated with T2DM in the dataset. In this section, each prospective parameter will be stratified by T2DM status to observe differences in the distribution. 

```{r}
#Convert outcome to binary factor class#
pima$Outcome <- as.factor(pima$Outcome)

#Box and whisker plot for pregnancies stratified by T2DM status#
p1<- pima %>% group_by(Outcome) %>% ggplot(aes(y=Pregnancies)) + geom_boxplot(aes(fill=Outcome)) + labs(title="Pregnancies Across Presentation of T2DM", xlab= "Presentation of T2DM", ylab = "Pregnancies") + theme(axis.text.x=element_blank()) + theme(legend.position = "none")

#Box and whisker plot for blood glucose stratified by T2DM status#
p2<- pima %>% group_by(Outcome) %>% ggplot(aes(y=Glucose)) + geom_boxplot(aes(fill=Outcome)) + labs(title="Blood Glucose Across Presentation of T2DM", xlab= "Presentation of T2DM", ylab = "Blood Glucose") + theme(axis.text.x=element_blank()) + theme(legend.position = "none")

#Box and whisker plot for BP stratified by T2DM status#
p3<- pima %>% group_by(Outcome) %>% ggplot(aes(y=BloodPressure)) + geom_boxplot(aes(fill=Outcome)) + labs(title="Blood Pressure Across Presentation of T2DM", xlab= "Presentation of T2DM", ylab = "Blood Pressure") + theme(axis.text.x=element_blank()) + theme(legend.position = "none")

#Box and whisker plot for skin thickness stratified by T2DM status#
p4<- pima %>% group_by(Outcome) %>% ggplot(aes(y=SkinThickness)) + geom_boxplot(aes(fill=Outcome)) + labs(title="Skin Thickness Across Presentation of T2DM", xlab= "Presentation of T2DM", ylab = "Skin Thickness") + theme(axis.text.x=element_blank()) + theme(legend.position = "none")

#arrange the plots in a 2x2#
grid.arrange(p1,p2,p3,p4,ncol=2)
```

From looking at the distribution of these parameters stratified by T2DM status, it appears that women who have had more pregnancies have a higher mean incidence of T2DM compared to women with fewer pregnancies. Additionally, the mean plasma glucose levels in those with T2DM is higher than in healthy patients. These two parameters may prove to be useful predictors in the logistic regression. Blood pressure and skin thickness have also been plotted as box and whisker plots, but these don't appear to have a distinct difference across T2DM status. 

```{r}
#Box and whisker plot for insulin levels stratified by T2DM status#
p5<- pima %>% group_by(Outcome) %>% ggplot(aes(y=Insulin)) + geom_boxplot(aes(fill=Outcome)) + labs(title="Insulin Levels Across Presentation of T2DM", xlab= "Presentation of T2DM", ylab = "Insulin Levels") + theme(axis.text.x=element_blank()) + theme(legend.position = "none")

#Box and whisker plot for BMI stratified by T2DM status#
p6<- pima %>% group_by(Outcome) %>% ggplot(aes(y=BMI)) + geom_boxplot(aes(fill=Outcome)) + labs(title="BMI Across Presentation of T2DM", xlab= "Presentation of T2DM", ylab = "BMI") + theme(axis.text.x=element_blank()) + theme(legend.position = "none")

#Box and whisker plot for pedigree levels stratified by T2DM status#
p7<- pima %>% group_by(Outcome) %>% ggplot(aes(y=DiabetesPedigreeFunction)) + geom_boxplot(aes(fill=Outcome)) + labs(title="Diabetes Pedigree Levels Across Presentation of T2DM", xlab= "Presentation of T2DM", ylab = "Pedigree Levels") + theme(axis.text.x=element_blank()) + theme(legend.position = "none")

#Box and whisker plot for ages stratified by T2DM status#
p8<- pima %>% group_by(Outcome) %>% ggplot(aes(y=Age)) + geom_boxplot(aes(fill=Outcome)) + labs(title="Age Across Presentation of T2DM", xlab= "Presentation of T2DM", ylab = "Age") + theme(axis.text.x=element_blank()) + theme(legend.position = "none")

#arrange the plots in a 2x2#
grid.arrange(p5,p6,p7,p8,ncol=2)
```

The latter set of parameters appear to have some differences as well. BMI and and age more clearly have a relationship that can be seen across the diabetes strata, but insulin levels and diabetes pedigree appear to be obsure. Ultimately, all of the parameters analysed will have to initially be put into a logistic regression to see which variables are statistically significant in their relationship with T2DM status. 
Not only should the variables be stratified by the outcome of interest, but there is high potential for confounding variables in this dataset. For example, the parameter `SkinThickness` is a measure of fat accumulation while BMI is also a common measure for load of the body. If these two variables have a high absolute correlation (>0.5), then they should be excluded from the logistic analysis. 

```{r}
#scatterplot for Skin Thickness on BMI across T2DM status#
pima %>% group_by(Outcome) %>% ggplot() + geom_point(aes(x=BMI, y=SkinThickness, colour=Outcome), alpha=0.5) + theme(legend.position = "none") + ggtitle("Cutaneous Thickness on BMI Across T2DM Patients")

#correlation plot between all variables in `pima`"
if(!require(corrplot)) install.packages("corrplot")
library(corrplot)
par( mfrow = c(1,1) )
pima$Outcome<- as.numeric(pima$Outcome)
cor1 <- cor(pima, method = c("pearson"))
cor2 <- round(cor(cor1),2)
corrplot::corrplot(cor2, method = "circle")
```

It appears that although the variables do have a correlation, none of them have an absolute value above 0.5, making all of them suitable for incorporation into the logistic regression. 

### Modeling

The data should now be split into training and test sets. The training set will be used to develop the regressions, and the test set will be to evaluate the accuracy of each regression. This will avoid overfitting the model to a dataset. The training and test sets will be split into an 80:20 divide, as the dataset has less than 800 observations. This 80:20 split will create a strong balance between parameter estimates and performance statistics. 

```{r}
if(!require(caret)) install.packages("caret")
library(caret)

#Attempt to convert the variables to class of factor#
pima[sapply(pima, is.character)] <- lapply(pima[sapply(pima, is.character)], as.factor)

#Split both datasets into an 80:20 split using a seed#
set.seed(1, sample.kind="Rounding")
sample_size<- floor(0.2*nrow(pima))
test_index <- sample(seq_len(nrow(pima)), size=sample_size)
pima_train<- pima[-test_index,]
pima_test<- pima[test_index,]
```

#### General Logistic Regression

Logistic regressions are models similar to linear regressions, but they are specialised for discrete rather than continuous outcomes. This makes them a perfect starting point for this study of classification algorithms, namely the binomial outcomes "Diabetic" or "non-Diabetic." 

```{r}
#Set the plots to a 2x2#
par(mfrow = c(2,2))
#Calculate the deviance residuals, coefficients, and significances#
pima_train$Outcome<- as.factor(pima_train$Outcome)
reg_glm <- glm(Outcome ~ ., data = pima_train, family = binomial(link = "logit"))
summary(reg_glm)
plot(reg_glm)
```

Another reason why the general logistic regression is much appreciated is because it shows which variables are significant in their relationship to T2DM outcome. From the table and plots above, it appears that regression intercept, `Pregnancies`, `Glucose`, `BMI`, and `DiabetesPedigreeFunction` are statistically significant to an alpha of almost 0. The variable `BloodPressure` is significant at the alpha=0.05. All of these parameters will be selected to continue to the optimised regression.

```{r}
#Optimised logistic regression with significant parameters#
par(mfrow = c(2,2))
reg_glm2 <- glm(Outcome~ Pregnancies+Glucose+BloodPressure+BMI + DiabetesPedigreeFunction, data=pima_train, family=binomial(link= "logit"))
summary(reg_glm2)
plot(reg_glm2)
```

In order to predict a binary outcome, bounds must be set. The regression continues in order to create a confusion matrix.

```{r}
pima_test$Outcome<- as.factor(pima_test$Outcome)
pred_glm <- predict(reg_glm2,pima_test, type = "response")
pred_glm <- ifelse(pred_glm <= 0.5, 1, 2)
pred_glm<- as.factor(pred_glm)
cm_glm<- confusionMatrix(pred_glm,pima_test$Outcome)
cm_glm
```

The confusion matrix generated with the general logistic regression has a balanced accuracy of 0.7818, which is actually quite impressive for an initial trial. The sensitivity of the model is high at 0.8713. However, the specificity leaves room to be desired at 0.6923. 

#### k-Nearest Neighbours Regression

The next model to be attempted will be the k-Nearest Neighbours algorithm. This model operates on the principle that test datapoints are similar to their "neighbouring" datapoints. These `k` values are used to develop the prediction. Compared to the general logistic regression, the kNN approach is a non-parametric model that is more supportive of non-linear models. 

```{r}
#Develop a model to test the accuracy on the number of neighbours#
reg_knn <- train(Outcome ~ ., data= pima_test, method = "knn", tuneGrid = data.frame(k = seq(1,20,1)))
reg_knn %>% ggplot()+geom_line(aes(x=k, y=Accuracy)) + labs(title= "Change in Regression Accuracy with varying optimal kNN")
```

According to the table and graph, the optimal number of neighbours for the model to search for is `k`=15. This value will be used in the prediction.

```{r}
pred_knn <- predict(reg_knn, pima_test, type="raw")
cm_knn<- confusionMatrix(pred_knn,pima_test$Outcome)
cm_knn
```

The kNN regression is shown to actually have a lower balanced accuracy compared to the generalised model, at 0.7247. Its sensitivity is slightly improved at 0.9109. However, the specificity is quite low at 0.5285. 

#### Random Forest Regression

This type of regression is a little more advanced, based on the concept of creating several "decision trees". This involves a branching equation to derive the output, making random forest models theoretically quite accurate, but very time-intensive.

```{r}
#Develop a random forest regression model to produce a confusion matrix#
reg_rf <- train(Outcome ~ ., method = "rf", data = pima_train)
pred_rf <- predict(reg_rf, pima_test)
pima_test$Outcome<- as.factor(pima_test$Outcome)
cm_rf<- confusionMatrix(pred_rf,pima_test$Outcome)
cm_rf
```

The random forest model shows a similar conclusion to the kNN regression. The balanced accuracy is 0.7183, the sensitivity is 0.8020, and the specificity is low at 0.6346. Its values, along with the kNN values, are inferior to the initial logistic regression. 

#### XGBoost Regression

Finally, the novel eXtreme Gradient Boosting software algorithm will be analysed. This machine-learning approach has gained several awards and traction on the web, and is the algorithm of choice for data science competitions on Kaggle. Its parallel tree boosting method is incredibly flexible and more time-efficient than the Random Forest algorithm. 

```{r}
# Develop an XGB regression model to produce a confusion matrix#
if(!require(xgboost)) install.packages("xgboost")
par(mfrow = c(2,1))
reg_xgb <- train(Outcome ~ ., method = "xgbTree", data = pima_test)
plot(reg_xgb)
pred_xgb <- predict(reg_xgb, pima_test)
cm_xgb<- confusionMatrix(pred_xgb,pima_test$Outcome)
cm_xgb
```

It appears that the XGBoost model was the superior approach compared to the three previously tested regression approaches. Its balanced accuracy is impressive at 0.8403, and its specificity is very high at 0.9307. Specificity has been shown to be lower in this modelling study, but this approach had the highest value at 0.7500. 

## Discussion

```{r}
# Create a knitr table of the regression approaches and their values#
cm<- data.frame(c("General Logistic Model", "k Nearest Neighbours", "Random Forest", "XGBoost"), c(cm_glm$byClass[1], cm_knn$byClass[1], cm_rf$byClass[1], cm_xgb$byClass[1]), c(cm_glm$byClass[2], cm_knn$byClass[2], cm_rf$byClass[2], cm_xgb$byClass[2]), c(cm_glm$byClass[11], cm_knn$byClass[11], cm_rf$byClass[11], cm_xgb$byClass[11]))
cm<- as_tibble(cm)
colnames(cm) <- c("Regression", "Sensitivity", "Specificity", "Balanced Accuracy")
cm %>% knitr::kable()
```

The analysis of the four regression approaches shows that the XGBoost was the most superior, with the highest sensitivity, specificity, and balanced accuracy. The general linear model has the next highest balanced accuracy, followed by the k Nearest Neighbours and the Random Forest Approaches. To further improve the analysis of this study, it would be recommended to increase the diversity of logistic regression algorithms tested. The `caret` package that was needed in this study holds a plethora of different approaches that may be suitable for this investigation. 

The results of this study are very timely, as being able to accurately predict which patients have high risk of being positive for T2DM is vital in robust prevention and treatment mechanisms. As the current health situation of the United States is unsteady due to the COVID-19 pandemic, this importance is only magnified for populations that face disproportionate gaps in health. This study will hopefully bring forward a greater call for investigation into disease incidence and comorbidities in a variety of Native American tribes, attempting to bridge long-overdue disparities. 

```{r}
knitr::purl()
```
